# Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation

Ruihong Qiu, Zi Huang, Hongzhi Yin*, and Zijian Wang

The University of Queensland

Brisbane, Australia

{r.qiu,h.yin1,zijian.wang}@uq.edu.au,huang@itee.uq.edu.au

# ABSTRACT

Recent advancements of sequential deep learning models such as Transformer and BERT have significantly facilitated the sequential recommendation. However, according to our study, the distribution of item embeddings generated by these models tends to degenerate into an anisotropic shape, which may result in high semantic similarities among embeddings. In this paper, both empirical and theoretical investigations of this representation degeneration problem are first provided, based on which a novel recommender model DuoRec is proposed to improve the item embeddings distribution. Specifically, in light of the uniformity property of contrastive learning, a contrastive regularization is designed for DuoRec to reshape the distribution of sequence representations. Given the convention that the recommendation task is performed by measuring the similarity between sequence representations and item embeddings in the same space via dot product, the regularization can be implicitly applied to the item embedding distribution. Existing contrastive learning methods mainly rely on data level augmentation for user-item interaction sequences through item cropping, masking, or reordering and can hardly provide semantically consistent augmentation samples. In DuoRec, a model-level augmentation is proposed based on Dropout to enable better semantic preserving. Furthermore, a novel sampling strategy is developed, where sequences having the same target item are chosen hard positive samples. Extensive experiments conducted on five datasets demonstrate the superior performance of the proposed DuoRec model compared with baseline methods. Visualization results of the learned representations validate that DuoRec can largely alleviate the representation degeneration problem.

# CCS CONCEPTS

- Information systems  $\rightarrow$  Recommender systems.

# KEYWORDS

sequential recommendation, contrastive learning

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

WSDM '22, February 21-25, 2022, Tempe, AZ, USA

$\odot$  2022 Association for Computing Machinery.

ACM ISBN 978-1-4503-9132-0/22/02...$15.00

https://doi.org/10.1145/3488560.3498433

# ACM Reference Format:

Ruihong Qiu, Zi Huang, Hongzhi Yin*, and Zijian Wang. 2022. Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22), February 21-25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3488560.3498433

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/d7943c64da5ebcfc67bd0d8005891024f2e86c607495070977ec70105b9fe60a.jpg)



(a) SASRec.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/15d7ac75e343a751c5dbf759ca7becdbf7d1aa8998dfa5814dd68012205cbb43.jpg)



(b) DuoRec.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/63469924bbd365819af83ac9b1ab09f965385cfd8496a8e3af3d4f799ef0ff8d.jpg)



(c) Singular values.



Figure 1: The item embedding matrix of the Amazon Clothing dataset is projected into 2D by SVD with colors indicating the frequency of items in the dataset. (a) Item embeddings learned by SASRec. Most of the rare items fall into a narrow cone, leading to high similarities among one another due to geometric properties. (b) Item embeddings learned by DuoRec. The distribution of item embeddings is more uniform in terms of both the magnitude and the frequency. (c) Normalized singular values of item embedding matrices. The fast decrease singular values of SASRec indicate the item embedding matrix is approximately in extreme low-rank. The slow decrease singular values of DuoRec reflect that the item embeddings are more representative.


# 1 INTRODUCTION

Traditional recommender systems usually predict a user's preference based on their historical records without considering the time factor [35, 37-39] while the preference generally shifts as time goes on. Recent sequential recommendation methods exploit sequential patterns of the user's interactions to capture the dynamic preference [3, 15, 19, 24-26, 31-34, 36, 41, 42, 51, 58].

During our study of these sequential models, a representation degeneration problem is observed in the item embeddings, whose distribution degenerates into a narrow cone and leads to an indiscriminate representation of the semantics. As shown in Figure 1(a), the item embeddings generated by SASRec [19] tend to be positive along the X-axis while distributing narrowly in the Y-axis direction.

In this situation, most of the items are positively related to one another due to the geometric properties. This distribution is typically anisotropic [1, 6, 7, 23, 45], reflected by which the singular values of the item embedding matrix quickly decrease to small values in the blue curve of Figure 1(c). There is a dominant dimension in the embedding matrix while other dimensions are ineffective, which is approximately in extreme low rank. In contrast, item embeddings of the proposed DuoRec are distributed more uniformly around the origin point with the singular values decreasing more slowly as in Figure 1(b) and the orange curve in Figure 1(c).

In this paper, we first investigate the causes of the representation degeneration with theoretical analysis, motivated by which a novel sequential recommender model DuoRec is proposed to improve the distribution of item embeddings. Specifically, inspired by the uniformity property of contrastive learning, a contrastive regularization is first designed to enhance the uniformity of the sequence representation distribution. Given that the recommendation is generally performed by measuring the similarity between sequence representations and item embeddings in the same space via dot product, the contrastive regularization can implicitly influence the item embeddings to distribute more uniformly. Existing contrastive learning methods generally generate positive samples using data-level augmentation, e.g., item cropping, masking, and reordering [51, 52], which may cause semantically inconsistent samples. Considering that the input data itself is generally embedded into a dense vector, we propose a model-level augmentation method, which applies two different sets of Dropout masks to the sequence representation learning. Moreover, since there are a large number of semantically similar sequences representing similar user preferences, an extra positive sampling strategy is developed to generate hard and informative positive samples, where sequences with the same target item are considered semantically similar.

The main contributions of this paper are summarized as follows:

- The representation degeneration problem is identified and investigated in sequential recommender models with theoretical and empirical analysis.

- To address the representation degeneration problem, a novel model DuoRec is proposed with a contrastive objective serving as the regularization over sequence representations.

- A model-level augmentation for user sequences is designed based on Dropout. Furthermore, a positive sampling strategy is developed using the target item as the supervision signal.

- Extensive experiments are conducted on five benchmark datasets, which show the state-of-the-art performance of the proposed DuoRec model and the effectiveness of the contrastive regularization for sequential recommendation.

# 2 REPRESENTATION DEGENERATION PROBLEM

# 2.1 Notations and Task Definition

In sequential recommendation, the problem setting is using historical interactions to infer user's preference and recommend the next item. There is an item set  $\mathcal{V}$  containing all items and  $|\mathcal{V}|$  is the number of items. The historical interactions of a user are constructed as an ordered list  $s = [v_{1}, v_{2}, \ldots, v_{t}]$ , where  $v_{i} \in \mathcal{V}, 0 \leq i \leq t$  and  $t$  indicates the current time step as well as the length of  $s$ . The

recommendation task is to predict the next item at time step  $t + 1$ , i.e.,  $v_{t+1}$  for the user. In the following, bold lowercase and uppercase symbols are used to denote vectors and matrices respectively.

# 2.2 Representation Degeneration Problem

To perform the next item prediction task in sequential recommendation, an interaction sequence is encoded into a fixed-length vector by the model to conduct a retrieval over the item set.

Given a sequence of items  $s = [v_1, v_2, \ldots, v_t]$ , the model calculates the probability of this sequence as  $p(s) = \prod_{n=1}^{t} p(v_n | c_n)$ , where  $c_n$  is the context of an interaction at time step  $n$ , containing all the previous interactions  $v_{<n}$ . The log probability of the sequence can be represented with a  $\theta$  parameterized model:

$$
\log p _ {\theta} (s) = \sum_ {n = 1} ^ {t} \log p _ {\theta} \left(v _ {n} \mid c _ {n}\right) = \sum_ {n = 1} ^ {t} \log \frac {\exp \left(\left\langle h _ {c} , v _ {n} \right\rangle\right)}{\sum_ {v ^ {\prime}} ^ {\mathcal {V}} \exp \left(\left\langle h _ {c} , v ^ {\prime} \right\rangle\right)}, \tag {1}
$$

where  $\pmb{h}_{\pmb{c}} \in \mathbb{R}^{d}$  is the  $d$ -dimension vector of the context and  $v_{n}, v' \in \mathbb{R}^{d}$  are the embeddings of item. Generally, the context is generated by sequential models such as GRU [5] and Transformer [44].

When using the cross-entropy loss to optimize the parameterized model above, the objective function can be abstracted as:

$$
J = - \mathbb {E} _ {s \sim p _ {\text {d a t a}}} [ \log p _ {\theta} (s) ]. \tag {2}
$$

According to [23, 53], in a well trained sequential model, the dot product term can be approximately decomposed as:

$$
\left\langle \boldsymbol {h} _ {c}, \boldsymbol {v} _ {n} \right\rangle \approx \log p \left(v _ {n} \mid c _ {n}\right) + \lambda_ {c _ {n}} = \operatorname {P M I} \left(v _ {n}, c _ {n}\right) + \log p \left(v _ {n}\right) + \lambda_ {c _ {n}}, \tag {3}
$$

where  $p(\cdot)$  is the true probability,  $\mathrm{PMI}(v_n, c_n) = \log \frac{p(v_n, c_n)}{p(v_n) p(c_n)}$  is the pointwise mutual information (PMI) between  $v_n$  and  $c_n$ , and  $\lambda_{c_n}$  is a context-related term. PMI captures the co-occurrence statistics between the variables, which is usually considered as semantics between tokens and the context.

To optimize the model with Equation (2), the gradient of the loss function with respect to an item embedding  $\boldsymbol{v}^*$  that does not appear in the input sequence is:

$$
\frac {\partial J}{\partial \boldsymbol {v} ^ {*}} = - \frac {\exp \left(\left\langle \boldsymbol {h} _ {c} , \boldsymbol {v} ^ {*} \right\rangle\right)}{\sum_ {\boldsymbol {v} ^ {\prime}} ^ {\mathcal {V}} \exp \left(\left\langle \boldsymbol {h} _ {c} , \boldsymbol {v} ^ {\prime} \right\rangle\right)} \boldsymbol {h} _ {c} \approx - p \left(v ^ {*} \mid c _ {n}\right) \boldsymbol {h} _ {c}. \tag {4}
$$

This gradient means that for those items with lower frequency in the dataset, the gradient direction is almost determined by the context vector. This is also reflected by Figure 1(a), where the yellow dots, representing items with a lower frequency, move in a similar direction within a narrow space. This is because most of the time, these item embeddings are trained with the gradient in Equation (4) as non-target items rather than being target items and trained with the gradient flowing through the encoder. Such a distribution of embeddings is referred to as an anisotropic space [7, 23].

As Equation (3) indicates, the semantic information between the sequence embedding and the item embedding is captured by dot product based on the co-occurrence. However, according to Equation (4), it is impractical to expect the sequence embeddings can present a clear difference in measuring the similarity between the rare items. Generally, the output layer of a recommender system is a dot product between the sequence representation and the item embeddings, which sets sequences and items representations in the

same latent space. In the following sections, a contrastive regularization will be introduced to relocate the distribution of sequence representations around the origin point, which implicitly improves the distribution of item embeddings.

# 3 PRELIMINARY: CONTRASTIVE LEARNING

# 3.1 Noise Contrastive Estimation

Contrastive learning is a training scheme that pulls the positive pairs of samples closer and pushes the negative pairs of samples away [9, 29]. Specifically, a Noise Contrastive Estimation (NCE) objective is generally applied to train an encoder  $f$ :

$$
\ell_ {\mathrm {N C E}} = \underset { \begin{array}{c} (x, x ^ {+}) \sim p _ {\text {p o s}} \\ x _ {i} ^ {- \text {i . i . d .}} \sim p _ {\text {d a t a}} \end{array} } {\mathbb {E}} \left[ - \log \frac {e ^ {f (x ^ {+}) ^ {\top} f (x) / \tau}}{e ^ {f (x ^ {+}) ^ {\top} f (x) / \tau} + \sum_ {i} e ^ {f (x _ {i} ^ {-}) ^ {\top} f (x) / \tau}} \right], \tag {5}
$$

where  $x$  and  $x^{+}$  are a pair of semantically close samples from the distribution  $p_{\mathrm{pos}}$ , which serve as the positive pair here while  $x$  and  $x_{i}^{-}$  are a pair of negative samples and  $x_{i}^{-}$  is sampled randomly from  $p_{\mathrm{data}}$ .  $\tau$  is the temperature parameter.

# 3.2 Alignment and Uniformity

The NCE loss is intuitively performing the push and pull game according to Equation (5). The mathematical descriptions are formally defined as the alignment and the uniformity of the representations under the assumption that vectors are normalized [46]:

$$
\ell_ {\text {a l i g n}} \triangleq \underset {(x, x ^ {+}) \sim p _ {\text {p o s}}} {\mathbb {E}} \| f (x ^ {+}) - f (x) \| ^ {2}, \tag {6}
$$

$$
\ell_ {\text {u n i f o r m}} \triangleq \log \underset {x ^ {- \mathrm {i . d .}}} {\mathbb {E}} p _ {\text {d a t a}} e ^ {- 2 \| f (x ^ {-}) - f (x) \| ^ {2}}, \tag {7}
$$

where  $p_{\mathrm{pos}}$  denotes the distribution of the positive pair of samples and  $p_{\mathrm{data}}$  is the distribution of the independent samples.

For Equation (6), minimizing  $\ell_{\mathrm{align}}$  is equivalent to encourage the learned representations of  $x$  and  $x^{+}$  from a positive pair distribution  $p_{\mathrm{pos}}$  to be close. For Equation (7), minimizing  $\ell_{\mathrm{uniform}}$  is equivalent to encourage the uniform distribution of the representations of all the data samples from the distribution  $p_{\mathrm{data}}$ .

# 4 METHOD:DUOREC

# 4.1 Sequence Encoding As User Representation

In sequential recommendation, the main idea is to aggregate the historical interactions to profile the user's preference. Similar to SASRec, the encoding module of DuoRec is a Transformer [44]. To leverage the encoding ability of Transformer, the items are firstly converted into embeddings. Then a multi-head self-attention module is applied to compute the user representation.

4.1.1 Embedding Layer. In DuoRec, there is an embedding matrix  $V \in \mathbb{R}^{|\mathcal{V}| \times d}$ , where  $d$  is the dimension of the embedding. For the input sequence  $s = [v_1, v_2, \dots, v_t]$ , the embedding representations are  $s = [v_1, v_2, \dots, v_t]$ , where  $v_*$  is the embedded vector.

To preserve the time order of the sequence, a positional encoding matrix  $P \in \mathbb{R}^{N \times d}$  is constructed, where  $N$  indicates the maximum length of all the sequences. Formally, the item embedding and the positional encoding are added up as the input vector for the

interaction at a time step  $t$  of the Transformer:

$$
\boldsymbol {h} _ {t} ^ {0} = \boldsymbol {v} _ {t} + \boldsymbol {p} _ {t}, \tag {8}
$$

where  $\pmb{h}_t^0 \in \mathbb{R}^d$  is the complete input vector of the interaction at  $t$  and  $\pmb{p}_t$  is the positional encoding of the time step  $t$ .

4.1.2 Self-attention Module. After obtaining the input sequences, the Transformer is applied to compute the updated representations of each item by the multi-head attention mechanism [44]. Assuming  $\pmb{H}^{0} = \left[\pmb{h}_{0}^{0},\pmb{h}_{1}^{0},\dots,\pmb{h}_{t}^{0}\right]$  is the hidden representation of the sequence as both the input of an  $L$ -layer multi-head Transformer encoder (Trm), the encoding procedure of the sequence can be defined as:

$$
\boldsymbol {H} ^ {L} = \operatorname {T r m} \left(\boldsymbol {H} ^ {0}\right), \tag {9}
$$

where the last hidden vector  $\pmb{h}_t^L$  in  $\pmb{H}^{L} = \left[\pmb{h}_{0}^{L},\pmb{h}_{1}^{L},\dots,\pmb{h}_{t}^{L}\right]$  is selected to be the user representation of this user sequence.

# 4.2 Recommendation Learning

The next item prediction task is framed as a classification task over the whole item set. Given the sequence representation  $\pmb{h}$  and the item embedding matrix  $V$ , the predictive score is calculated as:

$$
\hat {\boldsymbol {y}} = \text {s o f t m a x} (\boldsymbol {V h}), \tag {10}
$$

where  $\hat{\pmb{y}}\in \mathbb{R}^{|\mathcal{V}|}$ . With the index of the ground truth item converted into a one-hot vector  $\pmb{y}$ , the cross-entropy loss is calculated as:

$$
\ell_ {\text {R e c}} = - \text {o n e - h o t} \left(\boldsymbol {y} _ {i}\right) \log \left(\hat {\boldsymbol {y}} _ {i}\right). \tag {11}
$$

# 4.3 Contrastive Regularization

To alleviate the representation degeneration problem, a contrastive regularization is developed by the exploitation of both the unsupervised and the supervised contrastive samples.

4.3.1 Unsupervised Augmentation. The unsupervised contrastive augmentation in DuoRec aims to provide a semantically meaningful augmentation for individual sequences in an unsupervised style. In the previous method such as CL4SRec [51], the augmentation methods include item cropping, masking, and reordering. Similar techniques in natural language processing are applied, e.g., word deletion, reordering, and substitution [28, 49]. Although these methods provide augmentations that help to improve the performance of the corresponding models to some extent, the augmentations cannot provide a guarantee for high semantic similarity. Since the data-level augmentation is not perfectly fit for discrete sequence, a model-level augmentation is proposed in this paper. In the computation of a sequence vector, there are Dropout modules in both the embedding layer and the Transformer encoder. Forward-passing an input sequence twice with different Dropout masks will generate two different vectors, which are semantically similar while having different features. Therefore, we choose a different Dropout mask for the unsupervised augmentation of the input sequence  $s$ , which is firstly operated on the input embedding to the Transformer encoder in Equation (8) to obtain an  $h_t^{0'}$ . Afterward, the augmented input sequence embedding is fed into the Transformer encoder with the same weight but a different Dropout mask:

$$
\boldsymbol {H} ^ {L ^ {\prime}} = \operatorname {T r m} \left(\boldsymbol {H} ^ {0 ^ {\prime}}\right), \quad \boldsymbol {h} ^ {\prime} = \boldsymbol {h} _ {t} ^ {L ^ {\prime}} = \boldsymbol {H} ^ {L ^ {\prime}} [ - 1 ], \tag {12}
$$

where  $[-1]$  is mimicking the Python style of indexing the last element in the list and  $h^\prime$  is the augmented sequence representation.

4.3.2 Supervised Positive Sampling. The supervised contrastive augmentation in DuoRec aims to incorporate the semantic information between semantically similar sequences into the contrastive regularization. The reason why semantic positives are required is that if only unsupervised contrastive learning is applied, the originally semantic similar samples will be categorized into the negative samples [20]. Therefore, the most important thing is to determine what samples are semantically similar.

Semantic Similarity. In sequential recommendation, the goal is to predict users' preferences. If two sequences represent the same user preference, it is natural to infer these two sequences that contain the same semantics. Therefore, given two different user sequences  $s_i = \left[v_{i,1}, v_{i,2}, \ldots, v_{i,t^i}\right]$  and  $s_j = \left[v_{j,1}, v_{j,2}, \ldots, v_{j,t^j}\right]$ , if the predictive objectives of  $s_i$  and  $s_j$ , i.e.,  $v_{i,t^i + 1}$  and  $v_{j,t^j + 1}$ , are the same item,  $s_i$  and  $s_j$  are considered semantically similar in DuoRec.

Positive Sample. For the input sequence  $s$ , there are sequences having the same target item in the dataset. A semantic similar sequence  $s_s$  is randomly sampled from these sequences. With the input embedding  $\pmb{H}_s^{0'}$ , the supervised augmentation is:

$$
\boldsymbol {H} _ {s} ^ {L ^ {\prime}} = \operatorname {T r m} \left(\boldsymbol {H} _ {s} ^ {0 ^ {\prime}}\right), \quad \boldsymbol {h} _ {s} ^ {\prime} = \boldsymbol {h} _ {t, s} ^ {L ^ {\prime}} = \boldsymbol {H} _ {s} ^ {L ^ {\prime}} [ - 1 ], \tag {13}
$$

where  $h_s'$  is the augmented sequence representation.

4.3.3 Negative Sampling. To effectively construct the negative samples for an augmented pair of samples, all the other augmented samples in the same training batch are considered negative samples. Assuming that the training batch is  $\mathcal{B}$  and the batch size is  $|\mathcal{B}|$ , after the augmentation, there will be  $2|\mathcal{B}|$  hidden vectors,  $\left\{\pmb{h}_{1}',\pmb{h}_{1,s}',\pmb{h}_{2}',\pmb{h}_{2,s}',\dots,\pmb{h}_{|\mathcal{B}|}',\pmb{h}_{|\mathcal{B}|,s}'\right\}$ , where the subscript and superscript are overloaded to denote the index of samples in the batch and the augmentations for clarity. Therefore, for each positive pair in the batch, there are  $2(|\mathcal{B}| - 1)$  negative pairs as the negative set  $S^{-}$ . For example, for the augmented pair of sequence representations  $\pmb{h}_1'$  and  $\pmb{h}_{1,s}'$ , the corresponding negative set  $S_{1}^{-} = \left\{\pmb{h}_{2}',\pmb{h}_{2,s}',\pmb{h}_{3}',\pmb{h}_{3,s}',\dots,\pmb{h}_{|\mathcal{B}|}',\pmb{h}_{|\mathcal{B}|,s}'\right\}$ . If there are sequences with the same target item, these sequences will be removed from  $S^{-}$  as well.

4.3.4 Regularization Objective. Similar to Equation (5), the contrastive regularization for the batch  $\mathcal{B}$  in DuoRec is defined as:

$$
\begin{array}{l} \ell_ {\text {R e g}} = \underset {i \in | \mathcal {B} |} {\mathbb {E}} \left[ - \log \frac {e \left(\boldsymbol {h} _ {i} ^ {\prime}\right) ^ {\top} \left(\boldsymbol {h} _ {i , s} ^ {\prime}\right) / \tau}{e \left(\boldsymbol {h} _ {i} ^ {\prime}\right) ^ {\top} \left(\boldsymbol {h} _ {i , s} ^ {\prime}\right) / \tau + \sum_ {s ^ {-} \in S _ {i} ^ {-}} e \left(\boldsymbol {h} _ {i} ^ {\prime}\right) ^ {\top} \left(s ^ {-}\right) / \tau} \right] \\ + \underset {i \in | \mathcal {B} |} {\mathbb {E}} \left[ - \log \frac {e \left(\boldsymbol {h} _ {i , s} ^ {\prime}\right) ^ {\top} \left(\boldsymbol {h} _ {i} ^ {\prime}\right) / \tau}{e \left(\boldsymbol {h} _ {i , s} ^ {\prime}\right) ^ {\top} \left(\boldsymbol {h} _ {i} ^ {\prime}\right) / \tau + \sum_ {s ^ {-} \in S _ {i} ^ {-}} e \left(\boldsymbol {h} _ {i , s} ^ {\prime}\right) ^ {\top} \left(s ^ {-}\right) / \tau} \right], \tag {14} \\ \end{array}
$$

which computes twice for the unsupervised and the supervised augmented representation respectively.

Thus, the overall objective of DuoRec with  $\lambda$  scale weight is:

$$
\ell = \ell_ {\text {R e c}} + \lambda \ell_ {\text {R e g}}. \tag {15}
$$

# 4.4 Discussion

In this section, the properties of the contrastive regularization of DuoRec and the connection with other methods will be described.

4.4.1 Solving Representation Degeneration Problem. To investigate how the contrastive regularization can solve the representation degeneration problem, the property of the contrastive regularization  $\ell_{\mathrm{Reg}}$  in Equation (14) needs to be analyzed. According to Equation (6) and (7), the alignment and the uniformity of  $\ell_{\mathrm{Reg}}$  are as follows:

$$
\ell_ {\text {R e g , a l i g n}} \triangleq \underset {\left(\boldsymbol {h} _ {i} ^ {\prime}, \boldsymbol {h} _ {i, s} ^ {\prime}\right) \sim p _ {\text {p o s}}} {\mathbb {E}} \left\| \boldsymbol {h} _ {i} ^ {\prime} - \boldsymbol {h} _ {i, s} ^ {\prime} \right\| ^ {2}, \tag {16}
$$

$$
\ell_ {\text {R e g , u n i f o r m , f i r s t}} \triangleq \log \underset {s \stackrel {\text {i . d .}} {\sim} p _ {\text {d a t a}}} {\mathbb {E}} e ^ {- 2 \| \boldsymbol {h} _ {i} ^ {\prime} - \boldsymbol {s} \| ^ {2}}, \tag {17}
$$

$$
\ell_ {\text {R e g , u n i f o r m , s e c o n d}} \triangleq \log \underset {s \stackrel {\text {i . d .}} {\sim} p _ {\text {d a t a}}} {\mathbb {E}} e ^ {- 2 \| \boldsymbol {h} _ {i, s} ^ {\prime} - s \| ^ {2}}. \tag {18}
$$

In the alignment term, it is meaningful to keep the alignment between the positive pairs of representations from two augmentations of the same input sequence. While in the uniformity term, the objective is to uniformly distribute the representations of the sequences. The alignment between semantic positive pairs is pulling the representations of semantically similar sequences together. While the uniformity term is pushing all the sequence representations to be uniformly distributed. Since the main learning objective of recommendation is performed by the dot product between the sequence representation and the item embeddings in Equation (10), it is meaningful to regularize the distribution of the sequence representation so that the distribution of item embeddings can be influenced.

In the representation degeneration problem, an essential drawback of the cone distribution is that there is a dominant axis of the embeddings. Based on the uniformity, this situation will be eased because the sequence representation will be distributed uniformly, which will guide the distribution of the item embeddings via the dot product in Equation (10). While for the other drawback that rare words tend to be far away from the origin point,  $\ell_{\mathrm{Reg}}$  alleviates this phenomenon by adjusting the gradient for rare words. According to the analysis of the gradient in Equation (4) for rare words, the uni-direction of the gradient is because most of the rare words are mainly trained with the recommendation softmax loss. With the contrastive regularization, these rare words are exposed more often than before since there will be more positive and negative samplings, which are trained via the gradient flow through the encoder rather than directly on the embeddings.

4.4.2 Connection. Recent methods use the contrastive objective mainly for regularization. For example, CL4SRec [51] augments the input sequence in data-level with cropping, masking, and reordering. This is directly following the normal contrastive paradigm for computer vision to augment the samples in the input space. However, discrete sequences are hard to determine the semantic content and even harder to provide a semantically consistent augmentation. If the unsupervised Dropout augmentation of DuoRec is operated twice and only these unsupervised augmented samples are used for the contrastive regularization, it becomes the Unsupervised Contrastive Learning (UCL) variant in the following experiment.


Table 1: Statistics of the datasets after preprocessing.


<table><tr><td>Specs.</td><td>Beauty</td><td>Sports</td><td>Clothing</td><td>ML-1M</td><td>Yelp</td></tr><tr><td># Users</td><td>22,363</td><td>35,598</td><td>39,387</td><td>6,041</td><td>30499</td></tr><tr><td># Items</td><td>12,101</td><td>18,357</td><td>23,033</td><td>3,417</td><td>20068</td></tr><tr><td># Avg. Length</td><td>8.9</td><td>8.3</td><td>7.1</td><td>165.5</td><td>10.4</td></tr><tr><td># Actions</td><td>198,502</td><td>296,337</td><td>278,677</td><td>999,611</td><td>317182</td></tr><tr><td>Sparsity</td><td>99.93%</td><td>99.95%</td><td>99.97%</td><td>95.16%</td><td>99.95%</td></tr></table>

Since the augmentation of UCL avoids the data-level augmentations, which cannot guarantee the augmented samples still contain similar semantics, the UCL can outperform CL4SRec consistently. Similarly, if only the supervised augmentation of DuoRec is used, then it becomes the Supervised Contrastive Learning (SCL) variant, which can provide a harder training objective. And SCL can outperform UCL for using more appropriate samples. This is also observed by recent natural language processing research [8].

# 5 EXPERIMENT

In experiments, we will answer these research questions (RQ):

- RQ1: How does the DuoRec perform compared with the state-of-the-art methods? (Section 5.2)

- RQ2: How does the DuoRec perform compared with the existing contrastive training paradigms? (Section 5.3)

- RQ3: How does contrastive regularization help with the training? (Section 5.4)

- RQ4: How is the sensitivity of the hyper-parameters in the DuoRec model? (Section 5.5)

# 5.1 Setup

5.1.1 Dataset. The experiments are conducted over five benchmark datasets with statistics after preprocessing shown in Table 1.

- Amazon Beauty, Clothing and Sports [27]. Following baselines [19, 41, 51, 58], the widely used Amazon dataset is chosen in our experiments with three sub-categories.

- MovieLens-1M [12]. Following [41], the popular movie recommendation dataset is used here, denoted as ML-1M.

- Yelp, which is a widely used dataset for the business recommendation. Similar to [58], the transaction records after Jan. 1st, 2019 are used in our experiment.

Following [19, 41, 51, 58] for preprocessing, all interactions are considered as implicit feedback. Users or items appearing less than five times are removed. The maximum length of a sequence is 50.

5.1.2 Metrics. For overall evaluation, top-  $K$  Hit Ratio (HR@  $K$ ) and top-  $K$  Normalized Discounted Cumulative Gain (NDCG@  $K$ ) are applied with  $K$  chosen from  $\{5, 10\}$ . We evaluate the ranking results over the whole item set for the fair comparison [22].

5.1.3 Baselines. The following methods are used for comparison:

- BPR-MF [37] is the first method to use BPR loss to train a matrix factorization model.

- GRU4Rec [15] applies GRU to model the user sequence. It is the first recurrent model for sequential recommendation.

- Caser [42] is a CNN-based method capturing high-order patterns by applying horizontal and vertical convolutional operations for sequential recommendation.

- SASRec [19] is a single-directional self-attention model. It is a strong baseline in sequential recommendation.

- BERT4Rec [41] uses a masked item training scheme similar to the masked language model sequential in NLP. The backbone is the bi-directional self-attention mechanism.

-  $\mathbf{S}^3\mathrm{Rec}_{\mathrm{MIP}}$  [58] applied masked contrastive pre-training as well. The Mask Item Prediction (MIP) variant is used here.

- CL4SRec [51] uses item cropping, masking, and reordering as augmentations for contrastive learning. It is the most recent and strong baseline for sequential recommendation.

5.1.4 Implementation. The embedding size is set to 64 with all linear mapping functions in DuoRec has the same hidden size. The numbers of layers and heads in the Transformer are set to 2. The Dropout [40] rate on the embedding matrix and the Transformer module are chosen from  $\{0.1, 0.2, 0.3, 0.4, 0.5\}$ . The training batch size is set to 256. We use the Adam [21] optimizer with the learning  $0.001$ .  $\lambda$  in Equation (15) is chose from  $\{0.1, 0.2, 0.3, 0.4, 0.5\}$ .

# 5.2 Overall Performance

In this experiment, we evaluate the overall performance to compare DuoRec with the baselines, which is presented in Table 2.

According to the results, the first observation is that the non-sequential result, BPR-MF, can hardly achieve a comparable result with other sequential methods. When it comes to the deep learning era, the first representative method is GRU4Rec based on GRU, which can consistently outperform the non-sequential BPR-MF. It can be concluded that the incorporation of sequential information can improve performance. Similarly, Caser uses a convolutional module to aggregate the sequential tokens, which are stacked as a matrix. Caser generally has a similar performance to GRU4Rec. More recently, attention has become the strongest sequence encoder. SASRec is the first method to apply uni-directional attention for sequence encoding. Compared with the previous deep learning-based models, SASRec can improve the performance by a large margin. This is achieved by the more representative sequential encoder. More recent methods generally inherit the attention-based encoder while introducing extra objectives. For example, BERT4Rec applies the masked item prediction objective to enforce the model to understand the semantics by filling in the masks. Although such a task can introduce a meaningful signal for the model, the performance is not consistent since the masked item prediction is not aligned well with the recommendation task. A similar situation happens to  $S^3 \mathrm{Rec}_{\mathrm{MIP}}$ , which also relies on the masked item prediction as the pre-training objective. The finetuning stage gives out a more accurate prediction. For the most recent contrastive learning-based approach, CL4SRec, achieves a consistent improvement over the other baselines. The extra objective is the same as the normal contrastive learning norm to set two different views of the same sequence. For DuoRec, it can outperform all the baselines by a large


Table 2: Overall performance. Bold scores represent the highest results of all methods. Underlined scores stand for the highest results from previous methods. The DuoRec achieves the state-of-the-art result among all baseline models.


<table><tr><td>Dataset</td><td>Metric</td><td>BPR-MF</td><td>GRU4Rec</td><td>Caser</td><td>SASRec</td><td>BERT4Rec</td><td>\( S^3Rec_{MIP} \)</td><td>CL4SRec</td><td>DuoRec</td><td>Improv.</td></tr><tr><td rowspan="4">Beauty</td><td>HR@5</td><td>0.0120</td><td>0.0164</td><td>0.0259</td><td>0.0365</td><td>0.0193</td><td>0.0327</td><td>0.0401</td><td>0.0546±0.0013</td><td>35.91%</td></tr><tr><td>HR@10</td><td>0.0299</td><td>0.0365</td><td>0.0418</td><td>0.0627</td><td>0.0401</td><td>0.0591</td><td>0.0683</td><td>0.0845±0.0010</td><td>19.17%</td></tr><tr><td>NDCG@5</td><td>0.0040</td><td>0.0086</td><td>0.0127</td><td>0.0236</td><td>0.0187</td><td>0.0175</td><td>0.0223</td><td>0.0352±0.0006</td><td>57.85%</td></tr><tr><td>NDCG@10</td><td>0.0053</td><td>0.0142</td><td>0.0253</td><td>0.0281</td><td>0.0254</td><td>0.0268</td><td>0.0317</td><td>0.0443±0.0006</td><td>39.75%</td></tr><tr><td rowspan="4">Clothing</td><td>HR@5</td><td>0.0067</td><td>0.0095</td><td>0.0108</td><td>0.0168</td><td>0.0125</td><td>0.0163</td><td>0.0168</td><td>0.0193±0.0012</td><td>14.88%</td></tr><tr><td>HR@10</td><td>0.0094</td><td>0.0165</td><td>0.0174</td><td>0.0272</td><td>0.0208</td><td>0.0237</td><td>0.0266</td><td>0.0302±0.0009</td><td>11.03%</td></tr><tr><td>NDCG@5</td><td>0.0052</td><td>0.0061</td><td>0.0067</td><td>0.0091</td><td>0.0075</td><td>0.0101</td><td>0.0090</td><td>0.0113±0.0011</td><td>11.88%</td></tr><tr><td>NDCG@10</td><td>0.0069</td><td>0.0083</td><td>0.0098</td><td>0.0124</td><td>0.0102</td><td>0.0132</td><td>0.0121</td><td>0.0148±0.0008</td><td>12.12%</td></tr><tr><td rowspan="4">Sports</td><td>HR@5</td><td>0.0092</td><td>0.0137</td><td>0.0139</td><td>0.0218</td><td>0.0176</td><td>0.0157</td><td>0.0227</td><td>0.0326±0.0007</td><td>43.61%</td></tr><tr><td>HR@10</td><td>0.0188</td><td>0.0274</td><td>0.0231</td><td>0.0336</td><td>0.0326</td><td>0.0265</td><td>0.0374</td><td>0.0498±0.0009</td><td>33.16%</td></tr><tr><td>NDCG@5</td><td>0.0040</td><td>0.0096</td><td>0.0085</td><td>0.0127</td><td>0.0105</td><td>0.0098</td><td>0.0129</td><td>0.0208±0.0010</td><td>61.24%</td></tr><tr><td>NDCG@10</td><td>0.0051</td><td>0.0137</td><td>0.0126</td><td>0.0169</td><td>0.0153</td><td>0.0135</td><td>0.0184</td><td>0.0262±0.0008</td><td>42.39%</td></tr><tr><td rowspan="4">ML-1M</td><td>HR@5</td><td>0.0078</td><td>0.0763</td><td>0.0816</td><td>0.1087</td><td>0.0733</td><td>0.1078</td><td>0.1147</td><td>0.2038±0.0021</td><td>77.68%</td></tr><tr><td>HR@10</td><td>0.0162</td><td>0.1658</td><td>0.1593</td><td>0.1904</td><td>0.1323</td><td>0.1952</td><td>0.1975</td><td>0.2946±0.0018</td><td>49.16%</td></tr><tr><td>NDCG@5</td><td>0.0052</td><td>0.0385</td><td>0.0372</td><td>0.0638</td><td>0.0432</td><td>0.0616</td><td>0.0662</td><td>0.1390±0.0030</td><td>109.97%</td></tr><tr><td>NDCG@10</td><td>0.0079</td><td>0.0671</td><td>0.0624</td><td>0.0910</td><td>0.0619</td><td>0.0917</td><td>0.0928</td><td>0.1680±0.0032</td><td>81.03%</td></tr><tr><td rowspan="4">Yelp</td><td>HR@5</td><td>0.0127</td><td>0.0152</td><td>0.0156</td><td>0.0161</td><td>0.0186</td><td>0.0173</td><td>0.0216</td><td>0.0441±0.0006</td><td>104.17%</td></tr><tr><td>HR@10</td><td>0.0245</td><td>0.0263</td><td>0.0252</td><td>0.0265</td><td>0.0291</td><td>0.0282</td><td>0.0352</td><td>0.0631±0.0010</td><td>79.26%</td></tr><tr><td>NDCG@5</td><td>0.076</td><td>0.0104</td><td>0.0096</td><td>0.0102</td><td>0.0118</td><td>0.0114</td><td>0.0130</td><td>0.0325±0.0004</td><td>150.00%</td></tr><tr><td>NDCG@10</td><td>0.0119</td><td>0.0137</td><td>0.0129</td><td>0.0134</td><td>0.0171</td><td>0.0163</td><td>0.0185</td><td>0.0386±0.0005</td><td>108.65%</td></tr></table>

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/c9274792978abff587a67f7a7cc5d7b7ecb8f9dcc8a2641b4738c6e591f5eb21.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/3f8cc72eb32ab9bbb96b5b4c2ac57ae7cbe9fc391f2176f61b50bf05800600c0.jpg)



Figure 2: Performance of different contrastive objectives.


margin. The incorporation of the supervised and unsupervised positive samples can improve the overall performance by regularizing the distribution of the sequence and the item representation.

# 5.3 Ablation Study of Contrastive Learning

In this experiment, the efficacy of the unsupervised augmentation and the supervised positive sampling is evaluated. The variants are: CL4SRec, using cropping, masking, and reordering as augmentations to calculate NCE; UCL, the NCE uses unsupervised augmentations to optimize; SCL, the NCE uses supervised positive sampling to optimize; and UCL+SCL, trained with the addition of the UCL and the SCL losses. The result is shown in Figure 2.

From the result, it is clear that adding a contrastive objective can generally improve the recommendation performance compared

with the baseline SASRec. Compared with CL4SRec, UCL can outperform CL4SRec while both being unsupervised contrastive methods. It can be concluded that the model-level Dropout augmentation can provide a more semantically consistent unsupervised sample than the data-level augmentation. Furthermore, SCL relies on the target item to sample a semantically consistent supervised sample, which shows a large margin improvement over both the unsupervised methods. Interestingly, directly adding the UCL and the SCL losses will harm the performance. This could be due to the incompatible alignment of two contrastive losses. For DuoRec, both the unsupervised and supervised positive samples are exploited, which can yield the best performance compared with all other methods.

# 5.4 Contrastive Regularization in Training

To evaluate how the contrastive regularization affects the training, (1) the visualization of the learned embedding matrix and (2) the training losses, will be demonstrated to help understand how contrastive learning improves performance. The visualization is based on SVD decomposition, which will project the embedding matrix into 2D and give out the normalized singular values. The results are shown in Figure 3 and 4. The visualization of the training losses is decomposed into the alignment and the uniformity via Equation (6) and (7) as presented in Figure 5.

5.4.1 Visualization of Item Embedding. As discussed before, SASRec is trained without constraint on the embedding matrix, which yields a narrow cone in the latent space as in Figure 3(a) and 4(a). The resulted in singular values drastically decrease to very small values. Although CL4SRec has an extra contrastive loss with data-level augmentation, the embeddings improve in terms of the distribution magnitude while the rare items are still located on the same side

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/31131487d18ca58acc348653c267ae30b7a8bd5dbfc6b6f7ef4f331116c6b1c5.jpg)



(a) SASRec.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/a7066b4cda3819cd16794fb5f8d9545797fed4e118099f2c39d9eac4b6667586.jpg)



(b) CL4SRec.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/9adc48d913bf0233c4f5a76085f29ad738b18cf47d39f5e52a2db748aa2a4722.jpg)



(c) UCL.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/8f8c6f283f4eb043c747b5fe79aa1fabd8e2817f58b958847ebbc8f16f67a604.jpg)



(d) SCL.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/c5c46392275533d12e9511158d1ac0b4ff4a9fe4ea7356f5e9dd8593f296bc09.jpg)



(e) UCL+SCL.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/32bc3c9bd00b1b6b4556d9a5672687c95584db7cc438baac7cc913d26fb5489a.jpg)



(f) DuoRec.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/240b1d210ddecbd70e6f3218685b238a9096f593539492e8dc7537b4cc185d3e.jpg)



(g) Singular values.



Figure 3: Item embeddings on Clothing dataset.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/074bab572a5af2779c8dfc48f28d21464888cd62bb4d52b86e41b5c6a85c9658.jpg)



(a) SASRec.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/4f05848d1310ab1f0475edfdcd32ad12170f19e6179bf6b2b36c867525aebf54.jpg)



(b) CL4SRec.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/eec29623b4e9f4192e939e4d991a2f0a5728cc4feeb6dbfe99a4ff922348733a.jpg)



(c) UCL.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/0a906c6a7229fd4c4db1c7a0446eba2f9778653e5be3dcd0cbf83842c689a436.jpg)



(d) SCL.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/cefde759cfe695c84c2fef6c050666ae3c8d34cc1d7bcc6396bf832120e191eb.jpg)



(e) UCL+SCL.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/1b8a7e610f00c910592b28eed5ecec64da4bb58ad90e538b7f1c8b6552bd2248.jpg)



(f) DuoRec.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/2c0152befc289cf2d6cba842fcb4355d47103d46f5e6d9f9c1c58327b094f0f0.jpg)



(g) Singular values.



Figure 4: Item embeddings on Sports dataset.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/2173f4bd219dc9053641a2409ed64b793c6e6faa33dd2902f67de186a0a82cfb.jpg)



(a) Clothing.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/53733cbff2631f932ff9901ce511f8631a6d268a330064bf027f7921875649c0.jpg)



(b) Yelp.



Figure 5: Training loss with colors indicating the validation HR@5 (blue is better). The uniformity loss of sequence representations decreases during training of DuoRec as the HR@5 increases, which indicates a more uniform distribution. In contrast, the uniformity loss increases during training of CL4SRec, resulting in an anisotropic distribution.


of the origin point. And the singular values of CL4SRec decrease slower than SASRec. This could be due to the data-level augmentation cannot consistently provide reasonable sequences. While for the UCL variant, it generates a similar embedding distribution as CL4SRec since they are both based on unsupervised contrastive learning. While for SCL, which only uses supervised contrastive learning, it is clear that the distribution of embeddings is more balanced that both the high- and low-frequency items are located around the origin point. The singular values are significantly higher than the unsupervised methods. It can be concluded that the supervised positive samples are more semantically consistent with the input sequence. When adding both the unsupervised and the supervised positive samples, UCL+SCL has a similar situation as purely SCL for the Clothing dataset while different for the Sports dataset. This difference is due to the combination of the unsupervised and supervised contrastive losses, which could lead the model in a different training direction. For the DuoRec, the embedding is

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/e85792175e809ef39966e78d553a997333108f95f09ed704418021886254bda1.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/1e9cb45c0e66d5b1be50c890200c733fcd78c2b986f2bbee708f83061a7f0bfc.jpg)



Figure 6: Parameter sensitivity of Dropout ratio.


distributed in a balanced style with the singular values decrease slowly. The proper combination of unsupervised and supervised contrastive learning improves the item embedding distribution.

5.4.2 Alignment and Uniformity. To investigate how contrastive learning takes effect during the training, the alignment loss term and the uniformity loss term are illustrated (both are better when smaller). The uniformity is calculated within the original output sequence representation, which is in the same range of samples for every method. Noting that since the choices of the positive sample are different across different methods, the alignment term is shown as a trend indicator without a proper meaning for comparison.

From Figure 5(a) and 5(b), it is clear that as the training of DuoRec goes on, the uniformity loss decreases as the  $\mathrm{HR@5}$  increases. And the uniformity loss achieves a clearly lower value than CL4SRec, which reflects the sequence representations are distributed more uniformly. For the CL4SRec, the uniformity loss increases during the training, which indicates a worse distribution space compared with DuoRec. Although the alignment loss is slightly increasing during the training of both methods, the drop of uniformity loss actually improves the recommendation performance.

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/663bbbd440e8f1ecdb9e96cb78cebac087c8244ca908da5616d2fa217e7f95c0.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/3b7ce801f54df88afdb72688ccac6e81a4e27d3202e16f3733258db38c070560.jpg)



Figure 7: Parameter sensitivity of  $\lambda$ .


# 5.5 Parameter Sensitivity

In this experiment, the parameter sensitivity of the Dropout ratio for the augmentation and the  $\lambda$  in Equation (15) are investigated. The results are presented in Figure 6 and 7 respectively. More results for reproducibility can be found in the appendices.

The Dropout ratio mainly affects the unsupervised augmentation, which assumes that using different Dropout masks under the same weight can generate semantically similar samples. According to Figure 6, for ML-1M dataset, when increasing the Dropout ratio, the performance decreases, which could be due to the density for this dataset is higher with more training signals. When the augmentation is too different from the original input using a higher Dropout ratio, the model is guided to train in an inaccurate direction. While for the Beauty, Clothing, Sports and Yelp datasets, the effect of different Dropout ratios is not significant.

The  $\lambda$  in Equation (15) controls the scale of the contrastive regularization. According to Figure 7, the performance is consistent across different choices of  $\lambda$ . This is possibly because the contrastive regularization is well aligned with the recommendation task.

# 6 RELATED WORK

# 6.1 Sequential Recommendation

The sequential recommendation task is mainly related to the sequential modeling methods [3, 15, 17-19, 26, 41, 42, 51, 57, 58], which rely on recurrent neural networks such as GRU [5] or attention structures [44] as the sequence encoder. GRU4Rec [15] is the very first attempt to utilize the GRU network in sequential recommendation. Since the attention mechanism has shown a great ability, different related models are developed, e.g., SASRec [19], BERT4Rec [41] and  $S^3$  Rec [58]. Recent graph-based methods, FGNN [32, 34], GAG [36] and PosRec [31], achieve improved performance due to the graph modeling in sequence. In terms of the training method, most methods are based on the next-item prediction task [15, 19, 42]. These methods are naturally suitable for the next-item prediction problem. The other training scheme usually has extra training tasks [26, 41, 51, 58]. The pre-training tasks mainly have the masked item prediction, the masked attribute or segment prediction in  $S^3$  Rec [58] and the finetuning step is the same as the next-item prediction. A recent work MMInfoRec [33] applies an item level contrastive learning for feature-based sequential recommendation. For BERT4Rec [41] and CL4SRec [51], the auxiliary task is added in the multi-tasking style.

# 6.2 Contrastive Learning

Contrastive learning has been widely used in various deep learning areas for its strong ability to help with the self-supervised learning [2, 4, 8, 10, 11, 13, 14, 16, 43, 47]. For the computer vision problems, the early method such as CPC [14, 43] and DIM [16], the encodings of different scales of the same image are fed into the contrastive learning as positive pairs. In the follow-up methods e.g., MoCo [13], SimCLR [2] and SimSiam [4], the different augmentations of the same image are considered as positive pairs for the contrastive learning. For the video representation learning, MemDPC [10] applies a similar strategy as CPC and DIM to encode the feature vectors of the segment and the video clip as positive pairs. For the COCLR [11] method, the positive samples of a video clip in RGB space are determined by the closeness of clips in the optical flow space and vice versa. In contrastive learning in the language modeling, ConSERT [52] introduces traditional augmentation methods such as cropping and reordering into the sentence augmentation as positive pairs. SimCSE [8] treats the same sentence with different Dropout [40] masks as positive pairs.

Contrastive learning is used in recent recommendation methods. For the collaborative filtering methods, SGL [48] applies the NCE in node-level representation learning. SSL [54] proposes a siamese network to encode the items as pre-training with embedding-level augmentations. SEPT [55] applies the NCE for the socially-aware recommendation, which is based on node-level contrastive learning. For the contrastive learning in sequential recommendation,  $\mathrm{S}^3\mathrm{Rec}$  [58] incorporates the contrasting mechanism between the prediction and the ground truth of the attribute-level, the item-level, and the segment-level together for training. The segment-level contrastive learning is also applied by Ma et al. [26] as a multitasking objective. CL4SRec [51] proposes three augmentations for the interaction sequence and applies a similar contrastive strategy as MoCo [13] and SimCLR [2] to set these augmentations of the same sequence as the positive pair in training. DHCN [50] and MHCN [56] are graph-based methods with contrastive learning on node-level representation. A more recent work MMInfoRec [33] has achieved a great improvement in sequential recommendation with side information using a contrastive objective in item level via Dropout augmentation in the item embedding.

# 7 CONCLUSION

In this paper, the representation degeneration problem of the item embedding matrix in sequential recommendation is investigated. The empirical observation and the theoretical analysis are provided. To solve this problem, a DuoRec model is proposed, which contains a contrastive regularization with both the Dropout-based model-level augmentation and the supervised positive sampling to construct contrastive samples. The properties of this regularization term are analyzed towards the representation degeneration problem. Extensive experiments are conducted on five benchmark datasets, which verify the superiority of DuoRec. The visualization is demonstrated to show how DuoRec solves this problem.

# 8 ACKNOWLEDGMENTS

The work is supported by Australian Research Council (CE200100025, DP190102353, DP190101985, FT210100624).

# REFERENCES



[1] Daniel Bis, Maksim Podkorytov, and Xiuwen Liu. 2021. Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications. In NAACL-HLT.





[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. In ICML.





[3] Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen, Wen-Chih Peng, Xue Li, and Xiaofang Zhou. 2020. Sequence-Aware Factorization Machines for Temporal Predictive Analytics. In ICIDE.





[4] Xinlei Chen and Kaiming He. 2020. Exploring Simple Siamese Representation Learning. CoRR abs/2011.10566 (2020).





[5] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS.





[6] Kawin Ethayarajh. 2019. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. In EMNLP-ITCNLP.





[7] Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019. Representation Degeneration Problem in Training Natural Language Generation Models. In ICLR.





[8] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. CoRR abs/2104.08821 (2021).





[9] Michael Gutmann and Aapo Hyvarinen. 2012. Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics. J. Mach. Learn. Res. (2012).





[10] Tengda Han, Weidi Xie, and Andrew Zisserman. 2020. Memory-augmented Dense Predictive Coding for Video Representation Learning. In ECCV.





[11] Tengda Han, Weidi Xie, and Andrew Zisserman. 2020. Self-supervised Co-Training for Video Representation Learning. In NeurIPS.





[12] F. Maxwell Harper and Joseph A. Konstan. 2016. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. (2016).





[13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum Contrast for Unsupervised Visual Representation Learning. In CVPR.





[14] Olivier J. Henaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, and Aaron van den Oord. 2019. Data-Efficient Image Recognition with Contrastive Predictive Coding. CoRR abs/1905.09272 (2019).





[15] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In ICLR.





[16] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. 2019. Learning deep representations by mutual information estimation and maximization. In ICLR.





[17] Nguyen Quoc Viet Hung, Chi Thang Duong, Thanh Tam Nguyen, Matthias Weidlich, Karl Aberer, Hongzhi Yin, and Xiaofang Zhou. 2017. Argument discovery via crowdsourcing. VLDB J. (2017).





[18] Nguyen Quoc Viet Hung, Huynh Huu Viet, Thanh Tam Nguyen, Matthias Weidlich, Hongzhi Yin, and Xiaofang Zhou. 2018. Computing Crowd Consensus with Partial Agreement. IEEE Trans. Knowl. Data Eng. (2018).





[19] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Recommendation. In ICDM.





[20] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised Contrastive Learning. In NeurIPS.





[21] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR.





[22] Walid Krichene and Steffen Rendle. 2020. On Sampled Metrics for Item Recommendation. In SIGKDD.





[23] Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. 2020. On the Sentence Embeddings from Pre-trained Language Models. In EMNLP.





[24] Yang Li, Tong Chen, Yadan Luo, Hongzhi Yin, and Zi Huang. 2021. Discovering Collaborative Signals for Next POI Recommendation with Iterative Seq2Graph Augmentation. In IJCAI.





[25] Yang Li, Tong Chen, Peng-Fei Zhang, and Hongzhi Yin. 2021. Lightweight Self-Attentive Sequential Recommendation. CoRR abs/2108.1133 (2021).





[26] Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu. 2020. Disentangled Self-Supervision in Sequential Recommenders. In SIGKDD.





[27] Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-Based Recommendations on Styles and Substitutes. In SIGIR.





[28] Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, and Xia Song. 2021. COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining. CoRR abs/2102.08473 (2021).





[29] Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In NIPS.





[30] Ben Poole, Sherjil Ozair, Aaron van den Oord, Alex Alemi, and George Tucker. 2019. On Variational Bounds of Mutual Information. In ICML.





[31] Ruihong Qiu, Zi Huang, Tong Chen, and Hongzhi Yin. 2021. Exploiting Positional Information for Session-based Recommendation. CoRR abs/2107.00846 (2021).





[32] Ruihong Qiu, Zi Huang, Jingjing Li, and Hongzhi Yin. 2020. Exploiting Cross-session Information for Session-based Recommendation with Graph Neural Networks. ACM Trans. Inf. Syst. 38, 3 (2020), 22:1-22:23.





[33] Ruihong Qiu, Zi Huang, and Hongzhi Yin. 2021. Memory Augmented Multi-Instance Contrastive Predictive Coding for Sequential Recommendation. CoRR abs/2109.00368 (2021).





[34] Ruihong Qiu, Jingjing Li, Zi Huang, and Hongzhi Yin. 2019. Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks. In CIKM.





[35] Ruihong Qiu, Sen Wang, Zhi Chen, Hongzhi Yin, and Zi Huang. 2021. CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation. CoRR abs/2107.02390 (2021).





[36] Ruihong Qiu, Hongzhi Yin, Zi Huang, and Tong Chen. 2020. GAG: Global Attributed Graph Neural Network for Streaming Session-based Recommendation. In SIGIR.





[37] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI.





[38] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized Markov chains for next-basket recommendation. In WWW.





[39] Badrul Munir Sarwar, George Karypis, Joseph A. Konstan, and John Riedl. 2001. Item-based collaborative filtering recommendation algorithms. In WWW.





[40] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research (2014).





[41] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In CIKM.





[42] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In WSDM.





[43] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018).





[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Lion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NIPS.





[45] Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, and Quanquan Gu. 2020. Improving Neural Language Generation with Spectrum Control. In ICLR.





[46] Tongzhou Wang and Phillip Isola. 2020. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. In ICML.





[47] Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, and Mahsa Baktashmotlagh. 2021. Learning to Diversify for Single Domain Generalization. CoRR abs/2108.11726 (2021).





[48] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised Graph Learning for Recommendation. In SIGIR.





[49] Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. 2020. CLEAR: Contrastive Learning for Sentence Representation. CoRR abs/2012.15466 (2020).





[50] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Lizhen Cui, and Xiangliang Zhang. 2021. Self-Supervised Hypergraph Convolutional Networks for Session-based Recommendation. In AAAI.





[51] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, and Bin Cui. 2021. Contrastive Learning for Sequential Recommendation. CoRR abs/2010.14395 (2021).





[52] Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran Xu. 2020. ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer. In ACL/TJCNLP.





[53] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. 2018. Breaking the Softmax Bottleneck: A High-Rank RNN Language Model. In ICLR.





[54] Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix X. Yu, Aditya Krishna Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, and Evan Ettinger. 2020. Self-supervised Learning for Large-scale Item Recommendations. (2020).





[55] Junliang Yu, Hongzhi Yin, Min Gao, Xin Xia, Xiangliang Zhang, and Nguyen Quoc Viet Hung. 2021. Socially-Aware Self-Supervised Tri-Training for Recommendation. In SIGKDD.





[56] Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation. In WWW.





[57] Yan Zhang, Hongzhi Yin, Zi Huang, Xingzhong Du, Guowu Yang, and Defu Lian. 2018. Discrete Deep Learning for Fast Content-Aware Recommendation. In WSDM.





[58] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S-3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. In CIKM.



# A RESULTS OF DIFFERENT BATCH SIZES

For contrastive learning approaches, the sample size is an important hyper-parameter of the model since the sample size can affect the estimation of mutual information via the approximation of NCE [30]. In Figure 8, the experimental results of different batch sizes are presented. Under the choice from  $\{128, 256, 512, 1024, 2048\}$ , the performance has a slight decrease but in an acceptable range. This could be due to the multi-tasking learning paradigm, where the batch size affects not only the contrastive learning, but also the recommendation task.

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/d7027b07a4b2a37d270837d1b4c6376a53adff6aefffe0f40b8d9466e890742a.jpg)



Figure 8: Parameter sensitivity of batch size.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/0f57f45a16bcf1374866a71685ffe7375dfcc452a6e97d3d8ab2ba0ab19b89f3.jpg)


# B RESULTS OF DIFFERENT DROPOUT STRATEGIES

Since the Dropout plays an important role in the unsupervised augmentation, the effect of different Dropout strategies are evaluated, which are divided into the Dropout of embedding layer and the Dropout of hidden layers in Transformer. Both Dropout ratios are chosen from  $\{0.1, 0.2, 0.3, 0.4, 0.5\}$ . Results are demonstrated from Figure 9 to 13.

According to these results that the overall performance is more steady horizontally while fluctuates vertically, the effect of the Dropout of the hidden layers in Transformer has a stronger influence on the overall performance compared with the Dropout of embedding layer. Furthermore, the optimal choice of Dropout strategy on different datasets is different, which is also reflected by Figure 6.

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/85cedc559ce50c8e15f97d6f7f5dc6e81c6e5abb626dd6f6f68dff83fd5376f2.jpg)



(a)  $\mathrm{HR} @ 5$


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/df4aad459c15e7f03ad4e5b346e84f802c34a98702bc92dc69e90e357f944d46.jpg)



(b) HR@10.



Figure 9: Parameter sensitivity of Dropout ratio on Beauty.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/e9780196998c0009f58fd49f0741ba35f64fc742551a1c318b8169bafdaa1cc0.jpg)



(a) HR@5.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/d7b02bca4833eaffe8603f30ab441e32168672b38823b4e2b62fc8c52e33f07c.jpg)



(b) HR@10.



Figure 10: Parameter sensitivity of Dropout ratio on Clothing.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/9274d33ab99d709599d0a2ab709a28a0c67b83cddffe220e3ce87a9943bf52f6.jpg)



(a) HR@5.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/7956c3f8a9a8dbcffbd21a567780fd5388a7d9ac8db135a64db75a00ec662c7c.jpg)



(b) HR@10.



Figure 11: Parameter sensitivity of Dropout ratio on Sports.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/5f040723578ded73d31fffad422bbe3b4dcc644a09f96e614348bea1a1d6b198.jpg)



(a) HR@5.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/f1c4973b545a549ba901786f951ecbfc7f95c9c4efd98d42143d132cf6f9e0eb.jpg)



(b) HR@10.



Figure 12: Parameter sensitivity of Dropout ratio on ML-1M.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/b98f24fbbe5c9fb02996c411cbff127b4c254fa72e8e80a6323cfeea7ae04af8.jpg)



(a) HR@5.


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/5a1f34efb0d3dff378635fa333a36ad320a446eb0667c8c25f40474f63d63aff.jpg)



(b) HR@10.



Figure 13: Parameter sensitivity of Dropout ratio on Yelp.


# C RESULTS OF DIFFERENT TEMPERATURES

In the calculation of NCE, there is a temperature parameter  $\tau$  in Equation (14). This parameter would affect the scale of the estimation of mutual information. The evaluation of the effect of  $\tau$  chosen from  $\{0.1, 0.3, 0.6, 1, 3, 6\}$  is presented in Figure 14. It can be concluded that within a reasonable range, the overall performance of DuoRec is steady, although there is a slight drop on Beauty and Yelp datasets when  $\tau$  is small.

![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/c9022feea74f5528459902fb5c99c408f95c105363e47f29e9a04f811aa089ee.jpg)


![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-20/d864634d-d9bc-480d-8705-9041987769cb/7cb9714ec3a856c87db5317485ab3357b3fb27852d7830055ef936939f8bbc10.jpg)



Figure 14: Parameter sensitivity of temperature parameter.
